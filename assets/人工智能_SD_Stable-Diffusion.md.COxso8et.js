import{_ as t,c as e,o,ae as r}from"./chunks/framework.rTUm5mJw.js";const d="/blog/assets/repeat%E3%80%81epoch%E3%80%81batch_size.D_nZOk6Y.png",i="/blog/assets/ComfyUI-SVD.BsDJVUFI.png",m=JSON.parse('{"title":"Stable Diffusion","description":"","frontmatter":{},"headers":[],"relativePath":"人工智能/SD/Stable-Diffusion.md","filePath":"人工智能/SD/Stable-Diffusion.md"}'),l={name:"人工智能/SD/Stable-Diffusion.md"};function h(s,a,p,n,c,f){return o(),e("div",null,a[0]||(a[0]=[r('<h1 id="stable-diffusion" tabindex="-1">Stable Diffusion <a class="header-anchor" href="#stable-diffusion" aria-label="Permalink to &quot;Stable Diffusion&quot;">​</a></h1><h2 id="ai绘画的应用场景" tabindex="-1">AI绘画的应用场景 <a class="header-anchor" href="#ai绘画的应用场景" aria-label="Permalink to &quot;AI绘画的应用场景&quot;">​</a></h2><ul><li>人物 <ul><li>换脸</li><li>换装</li><li>换姿势</li><li>换表情</li></ul></li><li>抠图</li><li>局部重绘</li><li>放大</li><li>换背景</li><li>换颜色</li></ul><h2 id="stable-diffusion-1" tabindex="-1">Stable Diffusion <a class="header-anchor" href="#stable-diffusion-1" aria-label="Permalink to &quot;Stable Diffusion&quot;">​</a></h2><h3 id="sd的发布时间" tabindex="-1">SD的发布时间 <a class="header-anchor" href="#sd的发布时间" aria-label="Permalink to &quot;SD的发布时间&quot;">​</a></h3><p>1.0发布于2022年9月，2.0发布于2022年11月，3.0发布于2024年5月。</p><h3 id="sdxl简介" tabindex="-1">SDXL简介 <a class="header-anchor" href="#sdxl简介" aria-label="Permalink to &quot;SDXL简介&quot;">​</a></h3><p>SDXL 就是 SD 的升级版，图片生成的效果更好、更逼真、分辨率更高。</p><h3 id="sd的主要原理" tabindex="-1">SD的主要原理 <a class="header-anchor" href="#sd的主要原理" aria-label="Permalink to &quot;SD的主要原理&quot;">​</a></h3><p>将输入（文本或图片）向量化 ——&gt; 通过VAE压缩到Latent隐空间 ——&gt; 在隐空间中多次去噪 ——&gt; 通过VAE解码还原到像素空间</p><h3 id="从-u-net-到transformer" tabindex="-1">从 U-Net 到Transformer <a class="header-anchor" href="#从-u-net-到transformer" aria-label="Permalink to &quot;从 U-Net 到Transformer&quot;">​</a></h3><p>由于自注意力的可扩展性、强大的性能，扩散模型背后的骨干网络，已经由U-Net替换成了Transformer架构。Transformer 已然完成了深度学习领域的大一统。</p><h3 id="隐空间的作用" tabindex="-1">隐空间的作用 <a class="header-anchor" href="#隐空间的作用" aria-label="Permalink to &quot;隐空间的作用&quot;">​</a></h3><p>隐空间通俗来说将图片数据量压缩，这样便于提升训练和推理的性能。类似于视频帧的压缩。</p><h3 id="sd应用的主要文件夹" tabindex="-1">SD应用的主要文件夹 <a class="header-anchor" href="#sd应用的主要文件夹" aria-label="Permalink to &quot;SD应用的主要文件夹&quot;">​</a></h3><p>重点关注几个文件夹： checkpoint 大模型文件夹 、 embeddings 向量化、Lora微调、VAE变分自编码器、 ControlNet插件、Custom Nodes 自定义节点</p><h2 id="lora训练" tabindex="-1">LoRA训练 <a class="header-anchor" href="#lora训练" aria-label="Permalink to &quot;LoRA训练&quot;">​</a></h2><h3 id="repeat、epoch、batch-size的区别" tabindex="-1">repeat、epoch、batch_size的区别 <a class="header-anchor" href="#repeat、epoch、batch-size的区别" aria-label="Permalink to &quot;repeat、epoch、batch_size的区别&quot;">​</a></h3><p><img src="'+d+'" alt="repeat、epoch、batch_size"></p><h3 id="学习率" tabindex="-1">学习率 <a class="header-anchor" href="#学习率" aria-label="Permalink to &quot;学习率&quot;">​</a></h3><p>学习率类似于车速。学习率过高，学的太快，囫囵吞枣，可能会过拟合。学习率太低，可以更好看清路上的风景，也就是AI可以更细致的去学习，但是可能会导致不拟合，也就是出图完全不像，学的太慢了，并且花费的时间更多。</p><p>学习率默认值1e-4，即0.0001。</p><h3 id="打tag" tabindex="-1">打tag <a class="header-anchor" href="#打tag" aria-label="Permalink to &quot;打tag&quot;">​</a></h3><p><strong>如果是希望灵活调整的特征，都必须把tag打清楚</strong>。如果是固化在模型里面的特征，都不要去打tag。例如，如果希望头发颜色可以自定义，则必须写明 red hair ，如果不写明，将来AI会认为“人类的头发就是红色的”，将来所有生成的图片都将是红色头发。</p><h2 id="提示词" tabindex="-1">提示词 <a class="header-anchor" href="#提示词" aria-label="Permalink to &quot;提示词&quot;">​</a></h2><h3 id="权重语法" tabindex="-1">权重语法 <a class="header-anchor" href="#权重语法" aria-label="Permalink to &quot;权重语法&quot;">​</a></h3><p>SD权重语法汇总如下：</p><table tabindex="0"><thead><tr><th>语法</th><th>说明</th></tr></thead><tbody><tr><td><code>prompt</code></td><td>没有任何符号，默认情况，1倍的权重</td></tr><tr><td><code>(prompt)</code></td><td>增加权重，1层为1.1倍，2层为1.21倍，3层为1.331倍</td></tr><tr><td><code>(prompt: x)</code></td><td>自定义权重，x最好控制在0.4~1.6之间。太小容易被忽视，太大容易拟合图像出错</td></tr><tr><td><code>[prompt]</code></td><td>降低权重。1层中括号为0.9倍，3层中括号为0.729倍</td></tr><tr><td><code>{prompt}</code></td><td>轻微增加权重。1层花括号为1.05倍，三层大括号为1.15倍</td></tr><tr><td><code>[prompt1 and prompt2]</code></td><td>prompt1和prompt2的混合，例如<code>[hot and dog]</code>表示在火中的狗</td></tr><tr><td><code>[prompt1_prompt2]</code></td><td>组合语法。比如<code>[coffee_cake]</code>表示咖啡蛋糕，既不是生成咖啡，也不是生成蛋糕。</td></tr><tr><td><code>[prompt:0.8]</code></td><td>在80%时刻才开始生成prompt，前面的时间跟该prompt无关</td></tr><tr><td><code>[prompt::0.8]</code></td><td>前80%时间段生成prompt，后面的时间跟该prompt无关</td></tr><tr><td><code>[prompt1:prompt2:0.8]</code></td><td>前80%的时间渲染prompt1，后20%的时间渲染prompt2</td></tr><tr><td><code>[prompt1 | prompt2 | prompt3]</code></td><td>循环往复，交替采样。如果你想把两种，将多种东西融合成一种，比如颜色渐变。</td></tr><tr><td><code>[prompt1 break prompt2] </code></td><td>在两个提示词中间写上“break”，作用就是把两个词完全隔离开，避免提示词之间的污染。</td></tr><tr><td><code>prompt1 AND prompt2</code></td><td>组合提示词，AND必须大写。例如，如果想要头发又有紫色又有绿色，则 purple hair AND green hair。</td></tr></tbody></table><h2 id="sora" tabindex="-1">Sora <a class="header-anchor" href="#sora" aria-label="Permalink to &quot;Sora&quot;">​</a></h2><h3 id="sora简介" tabindex="-1">Sora简介 <a class="header-anchor" href="#sora简介" aria-label="Permalink to &quot;Sora简介&quot;">​</a></h3><p>Sora 由OpenAI发布于2024年2月份，一经发布就轰动了世界。</p><p>Sora是一种文生视频大模型，但OpenAI的野心不止于此，官网的副标题是“Word Simulator（世界模拟器）”。因为Sora在使用大量的视频训练的过程中，学习到了客观世界的物理规律，例如行走、自由落体、视角变换、近大远小等。</p><h3 id="sora有望推动机器人的发展" tabindex="-1">Sora有望推动机器人的发展 <a class="header-anchor" href="#sora有望推动机器人的发展" aria-label="Permalink to &quot;Sora有望推动机器人的发展&quot;">​</a></h3><p>Sora推出之所以轰动，不是因为它是文生视频，而是因为它通过观看大量的视频，理解了真实世界的物理规律。这就是“涌现”能力，除了文生视频带动视频相关行业的颠覆，还有两个更重要的价值： 通过AGI的途径之一（多种模态）、 直接用在机器人上。</p><h2 id="svd" tabindex="-1">SVD <a class="header-anchor" href="#svd" aria-label="Permalink to &quot;SVD&quot;">​</a></h2><h3 id="stable-video-diffusion简介" tabindex="-1">Stable Video Diffusion简介 <a class="header-anchor" href="#stable-video-diffusion简介" aria-label="Permalink to &quot;Stable Video Diffusion简介&quot;">​</a></h3><p>Stable Video Diffusion，简称SVD，由StabilityAI发布于2023年11月，开源，能够生成帧率 14、分辨率 576x1024 的视频。</p><p>SVD-XT是SVD 的微调升级版，分辨率不变，但能够生成帧率 25 的视频；</p><h3 id="svd的训练过程" tabindex="-1">SVD的训练过程 <a class="header-anchor" href="#svd的训练过程" aria-label="Permalink to &quot;SVD的训练过程&quot;">​</a></h3><p>先搜集了5.8亿个视频剪辑。经过层层筛选，最后保留了1.5亿个视频片段的超高质量数据集。</p><p>SVD的训练分为三个主要步骤： 文生图预训练、 视频生成预训练、 高质量视频微调。</p><h2 id="svd配合comfyui" tabindex="-1">SVD配合ComfyUI <a class="header-anchor" href="#svd配合comfyui" aria-label="Permalink to &quot;SVD配合ComfyUI&quot;">​</a></h2><p><img src="'+i+'" alt="ComfyUI-SVD"></p>',43)]))}const u=t(l,[["render",h]]);export{m as __pageData,u as default};
