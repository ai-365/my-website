<h1 id="top" style="font-size:40px;">大模型</h1>

<p><a style="position:fixed; " href="#top">回到顶部</a></p>

- [大模型](#大模型)
    - [什么是微调？](#什么是微调)
    - [什么是强化学习？](#什么是强化学习)
    - [Google IO 2024发布内容简介](#google-io-2024发布内容简介)
  - [大模型文件后缀名及其含义](#大模型文件后缀名及其含义)
  - [Huggingface](#huggingface)
    - [Hugging Face镜像站](#hugging-face镜像站)
    - [指定本地路径，避免反复下载](#指定本地路径避免反复下载)
  - [强化学习](#强化学习)
  - [深度学习](#深度学习)
  - [深度学习的基本概念](#深度学习的基本概念)
    - [残差连接](#残差连接)
    - [归一化](#归一化)
    - [线性层](#线性层)
    - [SoftMax](#softmax)
    - [激活函数](#激活函数)
    - [epoch](#epoch)
    - [repeat](#repeat)
    - [batch-size](#batch-size)
    - [过拟合和欠拟合的最通俗解释](#过拟合和欠拟合的最通俗解释)
  - [PyTorch](#pytorch)
    - [PyTorch相对Numpy的优势](#pytorch相对numpy的优势)
    - [将张量放入GPU环境](#将张量放入gpu环境)
  - [张量](#张量)
    - [张量简介](#张量简介)
    - [张量的元素类型](#张量的元素类型)
    - [特殊的张量](#特殊的张量)
    - [张量和Numpy数组的互转](#张量和numpy数组的互转)
    - [矩阵的乘法](#矩阵的乘法)


# 大模型

### 什么是微调？

从头开始训练模型，成本高昂。  对所有的参数进行微调，是低效的。 固定前面的层，只微调接近下游的那几层参数，效果又不好。  将高维映射到低维，然后经过一个线性层，再从低维还原到高维。

### 什么是强化学习？

通过与环境交互，根据反馈来学习最佳行为。 智能体评估当前的状态，采取相应的动作，根据动作的正确与否得到奖励或惩罚。  智能体通过与环境的互动，学习到一个策略，使奖励最大化。

###  Google IO 2024发布内容简介

基本围绕AI。Veo视频生成模型。Gemini 大模型。 Agent工具。AI辅助编程工具等。



## 大模型文件后缀名及其含义

（1）  safetensors

这是由 Hugging Face 推出的一种新型安全模型存储格式，特别关注模型安全性、隐私保护和快速加载。它仅包含模型的权重参数，而不包括执行代码，这样可以减少模型文件大小，提高加载速度。

（2） ckpt

PyTorch Lightning 框架采用的模型存储格式。它不仅包含了模型参数，还包括优化器状态以及可能的训练元数据信息，使得用户可以无缝地恢复训练或执行推理。

（3） bin

通常是一种通用的二进制格式文件，它可以用来存储任意类型的数据。但在某些情况下可用于存储原始二进制权重数据，加载时需额外处理。

（4） pth

是 PyTorch 中用于保存模型状态的标准格式。方便模型的持久化和复用，支持完整模型结构和参数的保存与恢复。



##  Huggingface

###  Hugging Face镜像站

国内网络环境无法使用Huggingface，可以使用其镜像站 ：https://hf-mirror.com

要下载镜像站的大模型文件，有两种方式。

方法一： 直接在网页下载

打开大模型页面 https://hf-mirror.com/用户名/模型名  ，进入files标签，直接下载里面的文件。

方法二： 使用命令行工具

首先安装命令行工具Python包：

```
pip  install  -U  huggingface_hub
```

然后设置环境变量  HF_ENDPOINT  =  "https://hf-mirror.com"

然后下载模型：

```
huggingface-cli  download  --resume-download  gpt2  --local-dir  gpt2
```

下载数据集：

```
huggingface-cli  download  --repo-type  drataset  --esume-download  wikitext  --local-dir  wikitext
```

注意，有些项目需要登录Huggingface Face源站获取Token才能下载。

### 指定本地路径，避免反复下载

大模型文件很大，每次都重新下载费时费力，因此可以指定本地路径。from_pretrain 函数可以接收一个模型的id，也可以接收模型的本地存储路径。

先在网络较好时将HF上的整个文件夹下载下来，然后推理的时候指定本地的这个文件夹即可。


## 强化学习

-  提示学习

插槽式、类似于插值字符串

-  语境学习

带有情感极性的句子示例

-  高效模型微调LoRA

在原本矩阵旁边添加低秩矩阵
在残差连接添加适配层作为可训练参数

-  低秩矩阵

秩序：矩阵中最大不相关向量的个数，可以理解为有秩序的程度。

矩阵的秩的度量其实就是矩阵的行列之间的相关性。如果矩阵的各行或列是线性无关的，矩阵就是满秩的。非零元素的行数或列数决定了秩的多少。

低秩矩阵：矩阵的秩相对矩阵的行数或列数而言很小。

图像处理中，秩可以理解为包含信息的丰富程度。如果一张草原图全是草，那么就是低秩。

-  稀疏矩阵

稀疏是指矩阵中非零元素的个数少。

-  思维链

解题思路和步骤输入给模型，使得模型不仅有结果，还有中间步骤。

## 深度学习

##  深度学习的基本概念

###  残差连接	

将输入多次处理后，再和最开始的那个输入做一次加法运算

###  归一化	

使一组数据限定在一个范围内，如0~1或-1~1，以加快收敛

### 线性层	

加入一组权重，然后不断训练调整

### SoftMax	

将一组值转为一组概率，总和为1

### 激活函数	

增加非线性，如果没有激活函数，就算层再多跟一层也没区别。目前最常用的是ReLu。

###  epoch

把整个数据集遍历一次称为一个epoch。推荐每运行完一个epoch就保存一次记录，这样就有更多的模型可供选择。

###  repeat

单张图片的重复遍历次数。在当前的epoch中，每张图片被遍历的次数。

###  batch-size

批次大小。较大的批次训练速度更快，但需要更大的显存，需要更多的批次才能收敛。较小的batch size训练较慢，但显存占用更小，模型收敛的更快。


### 过拟合和欠拟合的最通俗解释

欠拟合：没学会。
过拟合：学会了，但是学痴了，但能举一反三。

##  PyTorch

###  PyTorch相对Numpy的优势

PyTorch和Numpy的数组可以互相转换，本质上并无区别。PyTorch相对于Numpy的优势在于：
-  支持GPU并行加速
-  自动微分

所以，PyTorch 比 Numpy 快，而 Numpy 又比 Python 原生数组快！

###  将张量放入GPU环境

```
import torch 
print(torch.__version__)# torch版本
print(torch.version.cuda) #cuda版本
print(torch.cuda.is_available()) #cuda是否可用
```


代码	|	注释
--- 	|	---
device = torch.device("cuda")	|	使用GPU环境
device = torch.device("cpu")	|	使用CPU环境
A.device	|	判断对象在哪个环节
A= A.to(device)	|	放入device中
A.cpu().device	|	放入CPU中

torch.tensor(1.0) #标量
torch.tensor([[1.0,1.0],[1.0,1.0]])#二维数组 (2,2)

## 张量

### 张量简介

张量是PyTorch等深度学习框架中最基本的数据结构。张量的本质是多维数组，可以是一维的向量、二维的矩阵、三维的数组。

###  张量的元素类型

类型	|	CPU表示	|	GPU表示
---	|	---	|	---
32位短整型	|	torch.IntTensor	|	torch.cuda.intTensor
64位长整型	|	torch.LongTensor	|	torch.cuda.LongTensor
单精度浮点型	|	torch.FloatTensor	|	torch.cuda.FloatTensor
双精度浮点型	|	torch.DoubleTensor	|	torch.cuda.DoubleTensor

###  特殊的张量

代码	|	作用
---	|	---
torch.zeros(m,n)	|	mxn的全0矩阵
torch.ones(m,n)	|	mxn的全1矩阵
torch.eye(m,n)	|	mxn的单位矩阵，对角线为1，其它为0
torch.randn(m,n)	|	mxn的正态分布的随机数，0~1之间
torch.rand(m,n)	|	mxn的均匀分布的随机数，0~1之间
torch.Tensor([4,5,5,6])	|	根据数组字面量直接创建张量
torch.IntTensor([1,2])	|	指定数据类型

索引操作
y = x[0,  :  ]  第1行


###  张量和Numpy数组的互转

Numpy转Tensor： 

```
arr = np.ones(5,3)
T  = torch.from_numpy(a)
```

改变张量的形状

张量对象调用view()方法可以改变形状 ，例如：

```
import torch 
T1 = Torch.ones(3,8)
T2 = T1.view(4,6)
```

也可以将某个维度的长度设置为-1，会自动计算：

```
import torch 
T1 = Torch.ones(3,8)
T2 = T1.view(-1 ,6)   # 第一维自动计算
```

还有两个方法去掉或增加长度为1的维度：
-  squeeze(T) ： 去掉长度为1的维度
-  unsqueeze(T) ： 增加长度为1的维度

dataloader的参数

dataset	数据集
batch_size	批次大小，默认1

Epoch： 所有的样本都输入到模型中，称为一个epoch
Iteration： 一个Batch的样本输入到模型中，称为一个Iteration
batch_size： 一个批次的大小，一个Epoch=Batchsize*Iteration

DataLoader()的参数：
dataset	数据集
batch_size	批次大小
shuffle	是否乱序
sampler	样本采样函数，一般无需设置
batch_sampler	批次采样函数，一般无需设置
num_workers	使用多进程读取数据，使用的进程数
collate_fn	整理一个批次数据的函数
    


Dataset	数据集
DataLoader	数据装载器

自建数据集：
dataset = TensorDataset(torch.arange(1, 40))
dl = DataLoader(dataset, batch_size=10)   # 批次大小10，因此有4个批次


###  矩阵的乘法
T1.matmul(T2)
T1 @  T2  # 与上等价

神经网络和矩阵乘法的对应

神经网络和矩阵乘法的对应关系如下图所示：

    
    首先将输入展平为一个一维向量，中间的每条线对应着一个数值，这些数值组合起来就是一个矩阵，在深度学习中叫做权重。如果每个输入跟每个输出全部相连，就叫做全连接。
    
torch.nn
torch.nn是神经网络工具箱，该工具箱建立于Autogard（主要有自动求导和梯度反向传播功能），提供了网络搭建的模组，优化器等一系列功能。

搭建一个神经网络的整体流程：
    数组读取
    定义模型
    定义损失函数和优化器
    模型训练
    获取训练结果

Torch 运算
torch.item()	 取出数据，注意，只有一个元素时才能用
T.numpy()	转为Numpy
T.from_numpy()	从Numpy导入
T.view()	变形、重构尺寸，类
    似Numpy的reshape变形
T.transpose(0,1)	行列交换


拼接stack
torch.stack((A,B), dim=0) dim表示拼接方向


正向传播、反向传播

正向：由输入得到输出
反向：根据损失函数计算预测值与真实值之间的误差，计算每个节点对应的输入的梯度，最终得到参数的梯度信息。

梯度
为什么要算梯度？
使得损失函数最小，梯度（斜率）为0
y.backward() 计算梯度
x.grad 获取梯度值

梯度下降法是一种致力于找到函数极值点的算法。所谓“训练”或“学习”就是改进模型参数，以便通过大量训练步骤将损失最小化。梯度下降法的思路很简单，就是沿着函数下降最快的方向改变模型参数，直到到达最低点。

超参数
超参数就是需要用户手工配置的参数。

学习率
Wi+1  =  Wi  -  学习率 x  ▽Wi 
学习率就是每次下降的幅度。太小，则需要很多轮迭代，浪费资源； 太大，则可能会跳过最小点



一个简单的神经网络示例

我们拿一个最简单的FNN网络来对经典数据集diabetes糖尿病数据集来进行分类预测。

```        
    import numpy as np	
    import torch
    import matplotlib.pyplot as plt
    from torch.utils.data import Dataset, DataLoader
     
     
    # Prepare the dataset 准备数据集
    class DiabetesDateset(Dataset):
        # 加载数据集
        def __init__(self, filepath):
            xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32, encoding='utf-8')
            self.len = xy.shape[0]  # shape[0]是矩阵的行数,shape[1]是矩阵的列数
            self.x_data = torch.from_numpy(xy[:, :-1])
            self.y_data = torch.from_numpy(xy[:, [-1]])
     
        # 获取数据索引
        def __getitem__(self, index):
            return self.x_data[index], self.y_data[index]
     
        # 获得数据总量
        def __len__(self):
            return self.len
     
     
    dataset = DiabetesDateset('diabetes.csv')
    
    train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)  
    # num_workers为多线程
     
     
    # Define the model   定义模型
    
    class FNNModel(torch.nn.Module):
        def __init__(self):
            super(FNNModel, self).__init__()
            self.linear1 = torch.nn.Linear(8, 6)  # 输入数据的特征有8个,也就是有8个维度,随后将其降维到6维
            self.linear2 = torch.nn.Linear(6, 4)  # 6维降到4维
            self.linear3 = torch.nn.Linear(4, 2)  # 4维降到2维
            self.linear4 = torch.nn.Linear(2, 1)  # 2w维降到1维
            self.sigmoid = torch.nn.Sigmoid()  # 可以视其为网络的一层,而不是简单的函数使用
     
        def forward(self, x):
            x = self.sigmoid(self.linear1(x))
            x = self.sigmoid(self.linear2(x))
            x = self.sigmoid(self.linear3(x))
            x = self.sigmoid(self.linear4(x))
            return x
     
     
    model = FNNModel()
     
    # Define the criterion and optimizer 定义优化和损失
    criterion = torch.nn.BCELoss(reduction='mean')  # 返回损失的平均值
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
     
    epoch_list = []
    loss_list = []
     
    # Training  训练
    if __name__ == '__main__': 
        for epoch in range(100):
            # i是一个epoch中第几次迭代,一共756条数据,每个mini_batch为32,所以一个epoch需要迭代23次
            # data获取的数据为(x,y)
            loss_one_epoch = 0
            for i, data in enumerate(train_loader, 0):
                inputs, labels = data    # 输入实际
                y_pred = model(inputs)  #  预测输入
                loss = criterion(y_pred, labels)  # 预测实际
                loss_one_epoch += loss.item()
     
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            loss_list.append(loss_one_epoch / 23)
            epoch_list.append(epoch)
            print('Epoch[{}/{}],loss:{:.6f}'.format(epoch + 1, 100, loss_one_epoch / 23))
     
        # Drawing
        plt.plot(epoch_list, loss_list)
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.show()
```  




